{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c909981-4596-403c-a202-1330d29ce300",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import HuggingFacePipeline\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from langchain.prompts import PromptTemplate\n",
    "#from langchain_huggingface import HuggingFacePipeline\n",
    "# 1. åŠ è½½æœ¬åœ°æ¨¡å‹\n",
    "model_path = \"models/Qwen3-1.7B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, device_map=\"auto\", trust_remote_code=True)\n",
    "# 2. åˆ›å»ºæ–‡æœ¬ç”Ÿæˆpipeline\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=3000,\n",
    "    temperature=0.2,\n",
    "    do_sample=True\n",
    ")\n",
    "# 3. è½¬æ¢ä¸ºLangChainå¯ç”¨çš„LLM\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n",
    "template = \"\"\"<|im_start|>system\n",
    "ä½ æ˜¯ä¸€ä¸ªæ³•å¾‹AIåŠ©æ‰‹ï¼Œè¯·æ ¹æ®æä¾›çš„ä¸Šä¸‹æ–‡ä¿¡æ¯å›ç­”é—®é¢˜ã€‚è¯·ç›´æ¥å›ç­”é—®é¢˜ï¼Œä¸è¦è§£é‡Šæ¨ç†è¿‡ç¨‹ã€‚<|im_end|>\n",
    "<|im_start|>user\n",
    "ä¸Šä¸‹æ–‡ï¼š{context}\n",
    "\n",
    "é—®é¢˜ï¼š{question}<|im_end|>\n",
    "<|im_start|>assistant\n",
    "\"\"\"\n",
    "prompt = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58654d6f-f741-4570-af10-cb4fc9c18438",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.chains import LLMChain\n",
    "# åŠ è½½åŒä¸€ä¸ª embedding æ¨¡å‹ï¼ˆå¿…é¡»å’Œä¿å­˜æ—¶ä¸€è‡´ï¼‰\n",
    "EMBED_PATH = \"models/BAAI--bge-small-zh-v1.5\"\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=EMBED_PATH)\n",
    "# åŠ è½½å‘é‡æ•°æ®åº“ï¼Œå¹¶å…è®¸å±é™©çš„ååºåˆ—åŒ–\n",
    "vectorstore = FAISS.load_local(\"faiss_lawdb\", embedding_model, allow_dangerous_deserialization=True)\n",
    "qa_chain = LLMChain(prompt=prompt, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d2535a-bf2f-4460-8c8b-14977c1102d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_reference(docs: list) -> str:\n",
    "    references = []\n",
    "    for doc in docs:\n",
    "        meta = doc.metadata\n",
    "        law_type = meta.get(\"law_type\", \"æœªçŸ¥æ³•å¾‹\")\n",
    "        structure = meta.get(\"structure\", {})\n",
    "        bian = structure.get(\"ç¼–\", {}).get(\"åºå·\", \"\")\n",
    "        bian_title = structure.get(\"ç¼–\", {}).get(\"æ ‡é¢˜\", \"\")\n",
    "        zhang = structure.get(\"ç« \", {}).get(\"åºå·\", \"\")\n",
    "        zhang_title = structure.get(\"ç« \", {}).get(\"æ ‡é¢˜\", \"\")\n",
    "        jie = structure.get(\"èŠ‚\", {}).get(\"åºå·\", \"\")\n",
    "        jie_title = structure.get(\"èŠ‚\", {}).get(\"æ ‡é¢˜\", \"\")\n",
    "        # æ‹¼æ¥ç»“æ„å­—ç¬¦ä¸²\n",
    "        structure_str = f\"ç¬¬{bian}ç¼– {bian_title} ç¬¬{zhang}ç«  {zhang_title}\"\n",
    "        if jie:\n",
    "            structure_str += f\" ç¬¬{jie}èŠ‚ {jie_title}\"\n",
    "        content = doc.page_content\n",
    "        reference = f\"ã€Š{law_type}ã€‹ {structure_str}\\n{content}\"\n",
    "        references.append(reference)\n",
    "\n",
    "    return \"\\n\\n\".join(references)  # å¤šæ¡ä¹‹é—´ç©ºä¸¤è¡Œåˆ†éš”\n",
    "\n",
    "def extract_final_answer(response_text: str,docs) -> str:\n",
    "    # æå– assistant æ ‡ç­¾ä¹‹åçš„å†…å®¹ï¼ˆå¦‚æœæœ‰ï¼‰\n",
    "    if \"<|im_start|>assistant\" in response_text:\n",
    "        raw_an = response_text.split(\"<|im_start|>assistant\")[-1].strip()\n",
    "    else:\n",
    "        raw_an = response_text.strip()\n",
    "    # æå–è¢« <think> åŒ…å›´çš„å†…å®¹\n",
    "    import re\n",
    "    think_matches = re.findall(r\"<think>(.*?)</think>\", raw_an, re.DOTALL)\n",
    "    # å»é™¤ <think> æ ‡ç­¾åŠå…¶å†…å®¹ï¼Œä¿ç•™é think å†…å®¹\n",
    "    raw_an_without_think = re.sub(r\"<think>.*?</think>\", \"\", raw_an, flags=re.DOTALL).strip()\n",
    "    reference = build_reference(docs)\n",
    "    # æ‹¼æ¥å›ç­” + HTML æ ¼å¼å‚è€ƒæ³•æ¡\n",
    "    answer = (\n",
    "        f\"{raw_an_without_think}<br><div class='reference-box'>å‚è€ƒæ¡æ–‡ï¼š\\n{reference}</div>\"\n",
    "    )\n",
    "    return answer\n",
    "import gradio as gr\n",
    "custom_css = \"\"\"\n",
    ".reference-box {\n",
    "    background-color: #f0f4ff;  /* æ·¡è“è‰²èƒŒæ™¯ */\n",
    "    border-left: 4px solid #3b82f6;\n",
    "    padding: 8px;\n",
    "    margin-top: 6px;\n",
    "    border-radius: 4px;\n",
    "    font-size: 14px;\n",
    "    color: #333;\n",
    "}\n",
    "\"\"\"\n",
    "def gr_rag_chatbot(history, query):\n",
    "    docs = vectorstore.similarity_search(query, k=3)\n",
    "    context = \"\\n\".join([doc.page_content for doc in docs])\n",
    "    response = qa_chain.invoke({\"context\": context, \"question\": query})\n",
    "    raw_answer = response['text']\n",
    "    answer = extract_final_answer(raw_answer,docs)\n",
    "    # æ›´æ–°å¯¹è¯å†å²ï¼šå³è¾¹ç”¨æˆ·ï¼Œä¸‹é¢AIå›ç­”\n",
    "    history.append((f\"ğŸ‘¤ {query}\", f\"ğŸ¤– {answer}\"))\n",
    "    return history, \"\"  # è¿”å›å¯¹è¯å†…å®¹ & æ¸…ç©ºè¾“å…¥æ¡†\n",
    "with gr.Blocks(css=custom_css) as demo:\n",
    "    gr.Markdown(\"# ğŸ“š æ³•å¾‹åŠ©æ‰‹ Demo\")\n",
    "    chatbot = gr.Chatbot(label=\"å¯¹è¯è®°å½•\", height=600)\n",
    "    with gr.Row():\n",
    "        msg_input = gr.Textbox(\n",
    "            placeholder=\"è¯·è¾“å…¥ä½ çš„æ³•å¾‹é—®é¢˜ï¼Œä¾‹å¦‚ï¼šå©šå§»æ— æ•ˆçš„æƒ…å½¢æœ‰å“ªäº›ï¼Ÿ\",\n",
    "            show_label=False,\n",
    "            lines=3,\n",
    "            scale=4\n",
    "        )\n",
    "        send_btn = gr.Button(\"å‘é€\", scale=1)\n",
    "    state = gr.State([])  # ä¿å­˜å¯¹è¯å†å²\n",
    "    send_btn.click(\n",
    "        fn=gr_rag_chatbot,\n",
    "        inputs=[state, msg_input],\n",
    "        outputs=[chatbot, msg_input]\n",
    "    )\n",
    "demo.launch(server_name=\"0.0.0.0\", server_port=7860)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
